{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y, predictions):\n",
    "    N = y.size\n",
    "    accuracy = 1 - (np.count_nonzero(predictions-y)/N)\n",
    "    print(\"Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(weights, clean_features, parameters):\n",
    "    np.save('all/weights.npy', weights)\n",
    "    np.save('all/clean_features.npy', clean_features)\n",
    "    np.save('all/parameters.npy', parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Without feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'all/train.csv'\n",
    "labels, input_data, ids, features = load_csv_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DER_mass_MMC', 'DER_mass_transverse_met_lep', 'DER_mass_vis',\n",
       "       'DER_pt_h', 'DER_deltaeta_jet_jet', 'DER_mass_jet_jet',\n",
       "       'DER_prodeta_jet_jet', 'DER_deltar_tau_lep', 'DER_pt_tot',\n",
       "       'DER_sum_pt', 'DER_pt_ratio_lep_tau', 'DER_met_phi_centrality',\n",
       "       'DER_lep_eta_centrality', 'PRI_tau_pt', 'PRI_tau_eta',\n",
       "       'PRI_tau_phi', 'PRI_lep_pt', 'PRI_lep_eta', 'PRI_lep_phi',\n",
       "       'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_jet_num',\n",
       "       'PRI_jet_leading_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi',\n",
       "       'PRI_jet_subleading_pt', 'PRI_jet_subleading_eta',\n",
       "       'PRI_jet_subleading_phi', 'PRI_jet_all_pt'], dtype='<U27')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(input_data, labels, training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_tr, mean_tr, std_tr = extend_and_standardize(x_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 100\n",
    "gamma = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:70: RuntimeWarning: overflow encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims)\n",
      "/home/julien/Documents/Epfl/Courses/ML_course/projects/project1/scripts/implementations.py:6: RuntimeWarning: overflow encountered in square\n",
      "  return 1/2*np.mean(e**2)\n"
     ]
    }
   ],
   "source": [
    "losses_GD, ws_GD = least_squares_GD(y_tr, x_tr, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in greater\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/julien/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in less_equal\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "predictions_GD = predict_labels(w_GD,x_te)\n",
    "compute_accuracy(y_te,predictions_GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, w_LS = least_squares(y_tr,x_tr)\n",
    "predictions = predict_labels(w_LS,x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74402\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_te, _, _ = extend_and_standardize(x_te, mean_tr, std_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, w_LS = least_squares(y_tr,tx_tr)\n",
    "predictions = predict_labels(w_LS,tx_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.74468\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use cross-validation to find good hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "k_fold = 4\n",
    "k_indices = build_k_indices(y_tr, k_fold, seed)\n",
    "lambda_ = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n"
     ]
    }
   ],
   "source": [
    "lambdas, tr_losses, te_losses = find_optimal_lambda(y_tr,x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n"
     ]
    }
   ],
   "source": [
    "optimal_lambda = find_optimal_lambda(y_tr,x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rr = ridge_regression(y_tr,x_tr,optimal_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004520353656360241"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict_labels(w_rr,x_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7453000000000001\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(x_tr.shape[1])\n",
    "max_iters = 1000\n",
    "gamma = 0.01\n",
    "y_tr_log = np.ones(y_tr.size)\n",
    "y_tr_log[y_tr == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/Documents/Epfl/Courses/ML_course/projects/project1/scripts/implementations.py:116: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = (-y * np.log(pred) - (1 - y) * np.log(1 - pred)).mean()\n",
      "/home/julien/Documents/Epfl/Courses/ML_course/projects/project1/scripts/implementations.py:116: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = (-y * np.log(pred) - (1 - y) * np.log(1 - pred)).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=nan\n",
      "Current iteration=200, loss=nan\n",
      "Current iteration=300, loss=nan\n",
      "Current iteration=400, loss=nan\n",
      "Current iteration=500, loss=nan\n",
      "Current iteration=600, loss=nan\n",
      "Current iteration=700, loss=nan\n",
      "Current iteration=800, loss=nan\n",
      "Current iteration=900, loss=nan\n"
     ]
    }
   ],
   "source": [
    "loss, w_logistic = logistic_regression(y_tr_log, x_tr, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6876599999999999\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_labels(w_logistic, x_te)\n",
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julien/Documents/Epfl/Courses/ML_course/projects/project1/scripts/implementations.py:116: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = (-y * np.log(pred) - (1 - y) * np.log(1 - pred)).mean()\n",
      "/home/julien/Documents/Epfl/Courses/ML_course/projects/project1/scripts/implementations.py:116: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss = (-y * np.log(pred) - (1 - y) * np.log(1 - pred)).mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=nan\n",
      "Current iteration=100, loss=nan\n",
      "Current iteration=200, loss=nan\n",
      "Current iteration=300, loss=nan\n",
      "Current iteration=400, loss=nan\n",
      "Current iteration=500, loss=nan\n",
      "Current iteration=600, loss=nan\n",
      "Current iteration=700, loss=nan\n",
      "Current iteration=800, loss=nan\n",
      "Current iteration=900, loss=nan\n"
     ]
    }
   ],
   "source": [
    "loss, w_reg_logistic = reg_logistic_regression(y_tr_log, x_tr, optimal_lambda, initial_w, max_iters, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7079599999999999\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_labels(w_reg_logistic, x_te)\n",
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: EDA and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'all/train.csv'\n",
    "labels, input_data, ids, features = load_csv_data(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ratio = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = split_data(input_data, labels, training_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = x_tr, y_tr  # input_data, labels\n",
    "\n",
    "i, = np.where(features == 'PRI_jet_num')\n",
    "pri_jet_num_idx = np.squeeze(i)\n",
    "cond_null = X[:, pri_jet_num_idx] == 0\n",
    "cond_one = X[:, pri_jet_num_idx] == 1\n",
    "cond_plural = X[:, pri_jet_num_idx] >= 2\n",
    "conditions = [cond_null, cond_one, cond_plural]\n",
    "\n",
    "dsets = [X[cond] for cond in conditions]\n",
    "ybs = [y[cond] for cond in conditions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just remove any column with undefined -999 values. Also, before standardization, remove features with 0 variance. \n",
    "Second part: test how replacing -999 in DER_mass_MMC by defined mean affects the score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_dsets = []\n",
    "clean_features = []\n",
    "\n",
    "for dset in dsets:\n",
    "    \n",
    "    # Impute undefined DER_mass_MMC\n",
    "    \"\"\"\n",
    "    DER_mass_MMC = dset[:,0]\n",
    "    undefined_indices = (DER_mass_MMC == -999)\n",
    "    filter_undefined = DER_mass_MMC[~undefined_indices]\n",
    "    defined_mean = np.mean(filter_undefined)\n",
    "    print(defined_mean)\n",
    "    defined_median = np.median(filter_undefined)\n",
    "    print(defined_median)\n",
    "    DER_mass_MMC[undefined_indices] = defined_median\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove constant features and features with undefined samples\n",
    "    no_undefined = np.all(dset != -999, axis = 0)\n",
    "    no_constant = np.any(dset != dset[0], axis = 0)\n",
    "    cleaned = no_undefined * no_constant\n",
    "    clean_dset = dset[:,cleaned]\n",
    "    clean_dsets.append(clean_dset)\n",
    "    clean_features.append(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize and extend data, save mean and standard deviation of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = []\n",
    "standardized_dsets = []\n",
    "\n",
    "for clean_dset in clean_dsets:\n",
    "    standardized_dset, mean_x, std_x = extend_and_standardize(clean_dset)\n",
    "    # Added for testing purposes, handles outliers\n",
    "    \"\"\"\n",
    "    standardized_dset[standardized_dset > 3] = 3\n",
    "    standardized_dset[standardized_dset < -3]  = -3\n",
    "    \"\"\"\n",
    "    standardized_dsets.append(standardized_dset)\n",
    "    parameters.append((mean_x,std_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only extend datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [(0,1),(0,1),(0,1)]\n",
    "extended_dsets = []\n",
    "for clean_dset in clean_dsets:\n",
    "    tmp = build_model_data(clean_dset)\n",
    "    tmp_mean = np.mean(tmp, axis = 0)\n",
    "    tmp_std = np.std(tmp, axis = 0)\n",
    "    rows, cols = tmp.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            if tmp[row,col] > (tmp_mean[col]+3*tmp_std[col]):\n",
    "                tmp[row,col] = tmp_mean[col]+3*tmp_std[col]\n",
    "            elif tmp[row,col] < (tmp_mean[col]-3*tmp_std[col]):\n",
    "                tmp[row,col] = tmp_mean[col]-3*tmp_std[col]\n",
    "    extended_dsets.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ws_GD = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    initial_w = np.zeros(standardized_dset.shape[1])\n",
    "    losses_GD, w_GD = least_squares_GD(ybs[jet_num], standardized_dset, initial_w, max_iters, gamma)\n",
    "    ws_GD.append(w_GD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_GD, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75998\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second score, after replacing -999 in DER_mass_MMC by the defined mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72386\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: bit worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third score, after replacing -999 in DER_mass_MMC by the defined median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72848\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 1000\n",
    "gamma = 0.1\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_SGD = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    initial_w = np.zeros(standardized_dset.shape[1])\n",
    "    loss_SGD, w_SGD = least_squares_SGD(ybs[jet_num], standardized_dset, initial_w, batch_size, max_iters, gamma)\n",
    "    ws_SGD.append(w_SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_SGD, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75512\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_LS = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    loss, w = least_squares(ybs[jet_num],standardized_dset)\n",
    "    ws_LS.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_LS, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second score, after replacing -999 in DER_mass_MMC by the defined mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75986\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third score, after replacing -999 in DER_mass_MMC by the defined median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76336\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_LS = []\n",
    "for jet_num, extended_dset in enumerate(extended_dsets):\n",
    "    loss, w = least_squares(ybs[jet_num],extended_dset)\n",
    "    ws_LS.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76336\n"
     ]
    }
   ],
   "source": [
    "predictions = model_predictions(x_te, ws_LS, pri_jet_num_idx, clean_features, parameters)\n",
    "compute_accuracy(y_te, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 0\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n"
     ]
    }
   ],
   "source": [
    "lambdas = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    optimal_lambda = ridge_optimal_lambda(ybs[jet_num], standardized_dset)\n",
    "    lambdas.append(optimal_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_RR = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    w_RR = ridge_regression(ybs[jet_num],standardized_dset,lambdas[jet_num])\n",
    "    ws_RR.append(w_RR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(ws_RR, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_RR, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599400000000001\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second score, after replacing -999 in DER_mass_MMC by the defined mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75988\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third score, after replacing -999 in DER_mass_MMC by the defined median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75992\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.5981378276640935\n",
      "Current iteration=100, loss=0.4050067610901047\n",
      "Current iteration=200, loss=0.3970473245894675\n",
      "Current iteration=300, loss=0.39380211522480185\n",
      "Current iteration=400, loss=0.3922295351190929\n",
      "Current iteration=500, loss=0.3914176415322084\n",
      "Current iteration=600, loss=0.3909819194819656\n",
      "Current iteration=700, loss=0.39074228818318946\n",
      "Current iteration=800, loss=0.39060828411520365\n",
      "Current iteration=900, loss=0.39053243787160014\n",
      "Current iteration=1000, loss=0.390489123865655\n",
      "Current iteration=1100, loss=0.3904642228923078\n",
      "Current iteration=1200, loss=0.3904498357378845\n",
      "Current iteration=1300, loss=0.3904414918432804\n",
      "Current iteration=1400, loss=0.3904366389666473\n",
      "Current iteration=1500, loss=0.39043381039306674\n",
      "Current iteration=1600, loss=0.39043215900687506\n",
      "Current iteration=0, loss=0.6370571430238028\n",
      "Current iteration=100, loss=0.5437890177824172\n",
      "Current iteration=200, loss=0.5417063984969864\n",
      "Current iteration=300, loss=0.5411709012859485\n",
      "Current iteration=400, loss=0.5410006186761107\n",
      "Current iteration=500, loss=0.540942071715905\n",
      "Current iteration=600, loss=0.540921210086267\n",
      "Current iteration=700, loss=0.5409136370767896\n",
      "Current iteration=800, loss=0.5409108596055391\n",
      "Current iteration=0, loss=0.6052365641058394\n",
      "Current iteration=100, loss=0.5273157138250888\n",
      "Current iteration=200, loss=0.5248254137593048\n",
      "Current iteration=300, loss=0.5242492291326201\n",
      "Current iteration=400, loss=0.5240773367204362\n",
      "Current iteration=500, loss=0.5240187279063494\n",
      "Current iteration=600, loss=0.5239968518224285\n",
      "Current iteration=700, loss=0.5239881303500133\n",
      "Current iteration=800, loss=0.5239844678040677\n",
      "Current iteration=900, loss=0.5239828581401039\n"
     ]
    }
   ],
   "source": [
    "ws_LR = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    initial_w = np.zeros(standardized_dset.shape[1])\n",
    "    max_iters = 2000\n",
    "    gamma = 0.7 # 0.01\n",
    "    y_logistic = np.ones(ybs[jet_num].size)\n",
    "    y_logistic[ybs[jet_num] == -1] = 0\n",
    "    loss, w_LR = logistic_regression(y_logistic, standardized_dset, initial_w, max_iters, gamma)\n",
    "    ws_LR.append(w_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_LR, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76416\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(ws_RR, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second score, after replacing -999 in DER_mass_MMC by the defined mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76258\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third score, after replacing -999 in DER_mass_MMC by the defined median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76196\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76416\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Current iteration=0, loss=0.5985665553496784\n",
      "Current iteration=100, loss=0.40557929202525944\n",
      "Current iteration=200, loss=0.39793366841149214\n",
      "Current iteration=300, loss=0.3949550545132445\n",
      "Current iteration=400, loss=0.3935836792995515\n",
      "Current iteration=500, loss=0.3929015946370937\n",
      "Current iteration=600, loss=0.39254697900561397\n",
      "Current iteration=700, loss=0.3923579038077661\n",
      "Current iteration=800, loss=0.3922554613715885\n",
      "Current iteration=900, loss=0.39219934993675315\n",
      "Current iteration=1000, loss=0.39216837989391523\n",
      "Current iteration=1100, loss=0.3921511924385661\n",
      "Current iteration=1200, loss=0.39214161586665547\n",
      "Current iteration=1300, loss=0.3921362643594388\n",
      "Current iteration=1400, loss=0.3921332674227051\n",
      "Current iteration=1500, loss=0.3921315864040248\n",
      "Current iteration=0, loss=0.39318537128364117\n",
      "Current iteration=100, loss=0.39312705200197295\n",
      "Current iteration=0, loss=0.39127746572868966\n",
      "Current iteration=100, loss=0.3911715816908651\n",
      "Current iteration=200, loss=0.39116394937199617\n",
      "Current iteration=300, loss=0.39116052366495724\n",
      "Current iteration=400, loss=0.3911589592828559\n",
      "Current iteration=0, loss=0.3925390106136286\n",
      "Current iteration=100, loss=0.39245260058906795\n",
      "Current iteration=200, loss=0.3924467845718385\n",
      "Current iteration=300, loss=0.3924442854524574\n",
      "Iteration 1\n",
      "Current iteration=0, loss=0.3928892404225536\n",
      "Current iteration=100, loss=0.3927930835355233\n",
      "Current iteration=200, loss=0.39278636630247515\n",
      "Current iteration=300, loss=0.3927834165222419\n",
      "Current iteration=400, loss=0.3927819087693601\n",
      "Current iteration=0, loss=0.393833860445432\n",
      "Current iteration=100, loss=0.393776164930665\n",
      "Current iteration=0, loss=0.39191478527105983\n",
      "Current iteration=100, loss=0.3918080294347691\n",
      "Current iteration=200, loss=0.391799784091573\n",
      "Current iteration=300, loss=0.39179600718824703\n",
      "Current iteration=400, loss=0.3917942425705649\n",
      "Current iteration=0, loss=0.3931907795221109\n",
      "Current iteration=100, loss=0.39310605237432716\n",
      "Current iteration=200, loss=0.3931009355380356\n",
      "Current iteration=300, loss=0.3930988687198409\n",
      "Iteration 2\n",
      "Current iteration=0, loss=0.3937613596959776\n",
      "Current iteration=100, loss=0.3936620308919017\n",
      "Current iteration=200, loss=0.3936533844734197\n",
      "Current iteration=300, loss=0.3936493663110489\n",
      "Current iteration=400, loss=0.39364729383863023\n",
      "Current iteration=0, loss=0.3946969849383487\n",
      "Current iteration=100, loss=0.394639379735987\n",
      "Current iteration=0, loss=0.3927587947302227\n",
      "Current iteration=100, loss=0.39265223464169946\n",
      "Current iteration=200, loss=0.3926440769665426\n",
      "Current iteration=300, loss=0.3926404312786584\n",
      "Current iteration=400, loss=0.3926387716330433\n",
      "Current iteration=0, loss=0.39405962069353984\n",
      "Current iteration=100, loss=0.3939754306005397\n",
      "Current iteration=200, loss=0.39397045249975926\n",
      "Current iteration=300, loss=0.393968507901321\n",
      "Iteration 3\n",
      "Current iteration=0, loss=0.39491164844575116\n",
      "Current iteration=100, loss=0.3948055425591801\n",
      "Current iteration=200, loss=0.39479403659714735\n",
      "Current iteration=300, loss=0.3947885378249677\n",
      "Current iteration=400, loss=0.39478572714480836\n",
      "Current iteration=500, loss=0.3947842707182731\n",
      "Current iteration=0, loss=0.3958334303756862\n",
      "Current iteration=100, loss=0.39577593460639093\n",
      "Current iteration=0, loss=0.3938701645824074\n",
      "Current iteration=100, loss=0.39376403828578244\n",
      "Current iteration=200, loss=0.39375610570261477\n",
      "Current iteration=300, loss=0.393752677626714\n",
      "Current iteration=0, loss=0.39520208970976356\n",
      "Current iteration=100, loss=0.39511774343341544\n",
      "Current iteration=200, loss=0.3951128900379889\n",
      "Current iteration=300, loss=0.39511107924113725\n",
      "Iteration 4\n",
      "Current iteration=0, loss=0.3964122118654644\n",
      "Current iteration=100, loss=0.3962925377730174\n",
      "Current iteration=200, loss=0.39627654117085076\n",
      "Current iteration=300, loss=0.3962689099746981\n",
      "Current iteration=400, loss=0.39626512364823785\n",
      "Current iteration=500, loss=0.3962632330504161\n",
      "Current iteration=0, loss=0.39731171950163635\n",
      "Current iteration=100, loss=0.3972543432859241\n",
      "Current iteration=0, loss=0.3953162492094016\n",
      "Current iteration=100, loss=0.3952108581505325\n",
      "Current iteration=200, loss=0.3952033243874871\n",
      "Current iteration=300, loss=0.3952002117307841\n",
      "Current iteration=0, loss=0.3966855693426181\n",
      "Current iteration=100, loss=0.3965996159587593\n",
      "Current iteration=200, loss=0.3965948645628427\n",
      "Current iteration=300, loss=0.3965931997389979\n",
      "Iteration 5\n",
      "Current iteration=0, loss=0.3983421766746917\n",
      "Current iteration=100, loss=0.3981979199705242\n",
      "Current iteration=200, loss=0.398175283375642\n",
      "Current iteration=300, loss=0.39816484899960697\n",
      "Current iteration=400, loss=0.39815993749987416\n",
      "Current iteration=500, loss=0.3981576208308572\n",
      "Current iteration=0, loss=0.39920720682491084\n",
      "Current iteration=100, loss=0.39914998683345493\n",
      "Current iteration=0, loss=0.397171525354379\n",
      "Current iteration=100, loss=0.39706716934366343\n",
      "Current iteration=200, loss=0.3970602152502542\n",
      "Current iteration=300, loss=0.39705750817186475\n",
      "Current iteration=0, loss=0.3985835128359382\n",
      "Current iteration=100, loss=0.3984932555880123\n",
      "Current iteration=200, loss=0.3984885229341822\n",
      "Iteration 6\n",
      "Current iteration=0, loss=0.40078011123125556\n",
      "Current iteration=100, loss=0.4005986129274613\n",
      "Current iteration=200, loss=0.4005669632550511\n",
      "Current iteration=300, loss=0.40055333474883814\n",
      "Current iteration=400, loss=0.40054740450047366\n",
      "Current iteration=500, loss=0.4005448234437126\n",
      "Current iteration=0, loss=0.4015980005786351\n",
      "Current iteration=100, loss=0.40154105375153826\n",
      "Current iteration=0, loss=0.3995136503856443\n",
      "Current iteration=100, loss=0.3994105916061237\n",
      "Current iteration=200, loss=0.3994043942892671\n",
      "Current iteration=300, loss=0.39940216342214363\n",
      "Current iteration=0, loss=0.40097033429356804\n",
      "Current iteration=100, loss=0.40087182001307253\n",
      "Current iteration=200, loss=0.4008670221067612\n",
      "Iteration 7\n",
      "Current iteration=0, loss=0.40379711449385913\n",
      "Current iteration=100, loss=0.4035681032582035\n",
      "Current iteration=200, loss=0.40352569887965534\n",
      "Current iteration=300, loss=0.4035092439985032\n",
      "Current iteration=400, loss=0.40350281983116015\n",
      "Current iteration=500, loss=0.40350031001052417\n",
      "Current iteration=0, loss=0.4045595213803198\n",
      "Current iteration=100, loss=0.4045030921875515\n",
      "Current iteration=0, loss=0.40241804072253634\n",
      "Current iteration=100, loss=0.4023164421497562\n",
      "Current iteration=200, loss=0.4023112119572208\n",
      "Current iteration=0, loss=0.4039136788162776\n",
      "Current iteration=100, loss=0.4038041496690029\n",
      "Current iteration=200, loss=0.4037993438418015\n",
      "Iteration 8\n",
      "Current iteration=0, loss=0.40745567461460974\n",
      "Current iteration=100, loss=0.407171165680191\n",
      "Current iteration=200, loss=0.40711804037425475\n",
      "Current iteration=300, loss=0.40710015060526705\n",
      "Current iteration=400, loss=0.40709407356826105\n",
      "Current iteration=500, loss=0.4070919969547339\n",
      "Current iteration=0, loss=0.4081576767251296\n",
      "Current iteration=100, loss=0.4081019770497834\n",
      "Current iteration=0, loss=0.40595027868785316\n",
      "Current iteration=100, loss=0.4058500858794519\n",
      "Current iteration=200, loss=0.4058460116255293\n",
      "Current iteration=0, loss=0.4074619829348696\n",
      "Current iteration=100, loss=0.40735024861663877\n",
      "Current iteration=200, loss=0.40734609131615046\n",
      "Iteration 9\n",
      "Current iteration=0, loss=0.4118057258656251\n",
      "Current iteration=100, loss=0.41145715016567946\n",
      "Current iteration=200, loss=0.411395800050596\n",
      "Current iteration=300, loss=0.41137852928549534\n",
      "Current iteration=400, loss=0.41137360105300663\n",
      "Current iteration=0, loss=0.41244077447267735\n",
      "Current iteration=100, loss=0.41238682145611594\n",
      "Current iteration=0, loss=0.4101610673335823\n",
      "Current iteration=100, loss=0.41006315679167343\n",
      "Current iteration=200, loss=0.4100600488499621\n",
      "Current iteration=0, loss=0.41166807994686233\n",
      "Current iteration=100, loss=0.41156221688406636\n",
      "Current iteration=200, loss=0.4115591359773455\n",
      "Iteration 10\n",
      "Current iteration=0, loss=0.41689116257869946\n",
      "Current iteration=100, loss=0.41646523284424947\n",
      "Current iteration=200, loss=0.4164019874792986\n",
      "Current iteration=300, loss=0.4163880048809301\n",
      "Current iteration=400, loss=0.41638485661013297\n",
      "Current iteration=0, loss=0.4174523434519902\n",
      "Current iteration=100, loss=0.41740125135398115\n",
      "Current iteration=0, loss=0.41509882075716453\n",
      "Current iteration=100, loss=0.41500354725807426\n",
      "Current iteration=0, loss=0.4165860938883328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=0.41648703952167043\n",
      "Iteration 11\n",
      "Current iteration=0, loss=0.4227585135136648\n",
      "Current iteration=100, loss=0.42224333864045516\n",
      "Current iteration=200, loss=0.4221867905460583\n",
      "Current iteration=300, loss=0.42217773392650737\n",
      "Current iteration=0, loss=0.42323974414788235\n",
      "Current iteration=0, loss=0.4208141953117644\n",
      "Current iteration=100, loss=0.42072178664923826\n",
      "Current iteration=0, loss=0.4222654914291675\n",
      "Current iteration=100, loss=0.42217356164397674\n",
      "Iteration 12\n",
      "Current iteration=0, loss=0.429464120329188\n",
      "Current iteration=100, loss=0.42885398884242115\n",
      "Current iteration=200, loss=0.4288115805435419\n",
      "Current iteration=300, loss=0.4288071590378056\n",
      "Current iteration=0, loss=0.42986011840978683\n",
      "Current iteration=0, loss=0.427368780413748\n",
      "Current iteration=100, loss=0.4272796432202233\n",
      "Current iteration=0, loss=0.4287684237724266\n",
      "Current iteration=100, loss=0.428683773274004\n",
      "Iteration 13\n",
      "Current iteration=0, loss=0.43708688939412643\n",
      "Current iteration=100, loss=0.4363828005749566\n",
      "Current iteration=200, loss=0.4363572748977978\n",
      "Current iteration=0, loss=0.43739057768916517\n",
      "Current iteration=0, loss=0.4348458674252835\n",
      "Current iteration=100, loss=0.43476070936033734\n",
      "Current iteration=0, loss=0.43618045258227284\n",
      "Current iteration=100, loss=0.4361031714857362\n",
      "Iteration 14\n",
      "Current iteration=0, loss=0.44573583358758767\n",
      "Current iteration=100, loss=0.4449409085252664\n",
      "Current iteration=200, loss=0.44492916866750903\n",
      "Current iteration=0, loss=0.44593392637095497\n",
      "Current iteration=0, loss=0.44335581400699803\n",
      "Current iteration=0, loss=0.44461519529050575\n",
      "Iteration 15\n",
      "Current iteration=0, loss=0.4555444860869601\n",
      "Current iteration=100, loss=0.45465689028132567\n",
      "Current iteration=0, loss=0.4556150646472858\n",
      "Current iteration=0, loss=0.4530306565633608\n",
      "Current iteration=0, loss=0.4542084491253758\n",
      "Iteration 16\n",
      "Current iteration=0, loss=0.46665167607682045\n",
      "Current iteration=100, loss=0.4656608022672537\n",
      "Current iteration=0, loss=0.4665656119600214\n",
      "Current iteration=0, loss=0.4640072505044695\n",
      "Current iteration=0, loss=0.4651007933746623\n",
      "Iteration 17\n",
      "Current iteration=0, loss=0.47917183650561684\n",
      "Current iteration=0, loss=0.4788970391432707\n",
      "Current iteration=0, loss=0.47639991325173286\n",
      "Current iteration=0, loss=0.47740967787218697\n",
      "Iteration 18\n",
      "Current iteration=0, loss=0.4931565328816588\n",
      "Current iteration=0, loss=0.4926655439199016\n",
      "Current iteration=0, loss=0.4902652703696754\n",
      "Current iteration=0, loss=0.4911936902209295\n",
      "Iteration 19\n",
      "Current iteration=0, loss=0.5085543207369835\n",
      "Current iteration=0, loss=0.5078351402533703\n",
      "Current iteration=0, loss=0.5055653708005559\n",
      "Current iteration=0, loss=0.5064158908777341\n",
      "Iteration 20\n",
      "Current iteration=0, loss=0.5251788782747\n",
      "Current iteration=0, loss=0.5242481912443784\n",
      "Current iteration=0, loss=0.5221388984000919\n",
      "Current iteration=0, loss=0.5229148283612498\n",
      "Iteration 21\n",
      "Current iteration=0, loss=0.5426979814730681\n",
      "Current iteration=0, loss=0.5416126429182562\n",
      "Current iteration=0, loss=0.5396886396284353\n",
      "Current iteration=0, loss=0.5403924732760057\n",
      "Iteration 22\n",
      "Current iteration=0, loss=0.5606510406662565\n",
      "Current iteration=0, loss=0.5595126009329902\n",
      "Current iteration=0, loss=0.5577927328275684\n",
      "Current iteration=0, loss=0.5584255637330667\n",
      "Iteration 23\n",
      "Current iteration=0, loss=0.5784977377831089\n",
      "Current iteration=0, loss=0.5774437432243072\n",
      "Current iteration=0, loss=0.5759398214865222\n",
      "Current iteration=0, loss=0.5765014722568376\n",
      "Iteration 24\n",
      "Current iteration=0, loss=0.5956916596379433\n",
      "Current iteration=0, loss=0.5948680474016118\n",
      "Current iteration=0, loss=0.5935837467074638\n",
      "Current iteration=0, loss=0.5940736122476563\n",
      "Iteration 25\n",
      "Current iteration=0, loss=0.6117632923228252\n",
      "Current iteration=0, loss=0.6112764652339074\n",
      "Current iteration=0, loss=0.6102068424451749\n",
      "Current iteration=0, loss=0.6106249668678945\n",
      "Iteration 26\n",
      "Current iteration=0, loss=0.6263950124631661\n",
      "Current iteration=0, loss=0.6262462973581489\n",
      "Current iteration=0, loss=0.6253780736114709\n",
      "Current iteration=0, loss=0.6257262265810929\n",
      "Iteration 27\n",
      "Current iteration=0, loss=0.6394745389657451\n",
      "Current iteration=0, loss=0.6394825833372977\n",
      "Current iteration=0, loss=0.6387956774503032\n",
      "Current iteration=0, loss=0.6390779393257446\n",
      "Iteration 28\n",
      "Current iteration=0, loss=0.6511188330026457\n",
      "Current iteration=0, loss=0.6508373487550794\n",
      "Current iteration=0, loss=0.6503075057892841\n",
      "Current iteration=0, loss=0.6505300161302822\n",
      "Iteration 29\n",
      "Current iteration=0, loss=0.6616716699559294\n",
      "Current iteration=100, loss=0.6694184982560233\n",
      "Current iteration=200, loss=0.6815509973550603\n",
      "Current iteration=300, loss=0.6816294784806237\n",
      "Current iteration=400, loss=0.6816297412859397\n",
      "Current iteration=500, loss=0.6816297421634206\n",
      "Current iteration=600, loss=0.6816297421663507\n",
      "Current iteration=700, loss=0.6816297421663611\n",
      "Current iteration=800, loss=0.6816297421663611\n",
      "Current iteration=900, loss=0.6816297421663611\n",
      "Current iteration=1000, loss=0.6816297421663611\n",
      "Current iteration=1100, loss=0.6816297421663611\n",
      "Current iteration=1200, loss=0.6816297421663611\n",
      "Current iteration=1300, loss=0.6816297421663611\n",
      "Current iteration=1400, loss=0.6816297421663611\n",
      "Current iteration=1500, loss=0.6816297421663611\n",
      "Current iteration=1600, loss=0.6816297421663611\n",
      "Current iteration=1700, loss=0.6816297421663611\n",
      "Current iteration=1800, loss=0.6816297421663611\n",
      "Current iteration=1900, loss=0.6816297421663611\n",
      "Current iteration=0, loss=0.6814337935203937\n",
      "Current iteration=100, loss=0.6792807690737225\n",
      "Current iteration=200, loss=0.6792666622883827\n",
      "Current iteration=300, loss=0.6792665609460757\n",
      "Current iteration=400, loss=0.6792665602175987\n",
      "Current iteration=500, loss=0.679266560212362\n",
      "Current iteration=600, loss=0.6792665602123239\n",
      "Current iteration=700, loss=0.6792665602123238\n",
      "Current iteration=800, loss=0.6792665602123238\n",
      "Current iteration=900, loss=0.6792665602123238\n",
      "Current iteration=1000, loss=0.6792665602123238\n",
      "Current iteration=1100, loss=0.6792665602123238\n",
      "Current iteration=1200, loss=0.6792665602123238\n",
      "Current iteration=1300, loss=0.6792665602123238\n",
      "Current iteration=1400, loss=0.6792665602123238\n",
      "Current iteration=1500, loss=0.6792665602123238\n",
      "Current iteration=1600, loss=0.6792665602123238\n",
      "Current iteration=1700, loss=0.6792665602123238\n",
      "Current iteration=1800, loss=0.6792665602123238\n",
      "Current iteration=1900, loss=0.6792665602123238\n",
      "Current iteration=0, loss=0.6788192281262477\n",
      "Current iteration=100, loss=0.675238833683361\n",
      "Current iteration=200, loss=0.6751902438186739\n",
      "Current iteration=300, loss=0.6751894535315341\n",
      "Current iteration=400, loss=0.6751894406428183\n",
      "Current iteration=500, loss=0.6751894404326082\n",
      "Current iteration=600, loss=0.6751894404291796\n",
      "Current iteration=700, loss=0.6751894404291243\n",
      "Current iteration=800, loss=0.6751894404291234\n",
      "Current iteration=900, loss=0.6751894404291234\n",
      "Current iteration=1000, loss=0.6751894404291234\n",
      "Current iteration=1100, loss=0.6751894404291234\n",
      "Current iteration=1200, loss=0.6751894404291234\n",
      "Current iteration=1300, loss=0.6751894404291234\n",
      "Current iteration=1400, loss=0.6751894404291234\n",
      "Current iteration=1500, loss=0.6751894404291234\n",
      "Current iteration=1600, loss=0.6751894404291234\n",
      "Current iteration=1700, loss=0.6751894404291234\n",
      "Current iteration=1800, loss=0.6751894404291234\n",
      "Current iteration=1900, loss=0.6751894404291234\n",
      "Current iteration=0, loss=0.6757778701252214\n",
      "Current iteration=100, loss=0.6804424643985594\n",
      "Current iteration=200, loss=0.6804716296410392\n",
      "Current iteration=300, loss=0.680471777213763\n",
      "Current iteration=400, loss=0.6804717779595938\n",
      "Current iteration=500, loss=0.6804717779633627\n",
      "Current iteration=600, loss=0.6804717779633819\n",
      "Current iteration=700, loss=0.6804717779633819\n",
      "Current iteration=800, loss=0.6804717779633819\n",
      "Current iteration=900, loss=0.6804717779633819\n",
      "Current iteration=1000, loss=0.6804717779633819\n",
      "Current iteration=1100, loss=0.6804717779633819\n",
      "Current iteration=1200, loss=0.6804717779633819\n",
      "Current iteration=1300, loss=0.6804717779633819\n",
      "Current iteration=1400, loss=0.6804717779633819\n",
      "Current iteration=1500, loss=0.6804717779633819\n",
      "Current iteration=1600, loss=0.6804717779633819\n",
      "Current iteration=1700, loss=0.6804717779633819\n",
      "Current iteration=1800, loss=0.6804717779633819\n",
      "Current iteration=1900, loss=0.6804717779633819\n",
      "Iteration 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6372057644955702\n",
      "Current iteration=100, loss=0.5438416813162126\n",
      "Current iteration=200, loss=0.5417860172541333\n",
      "Current iteration=300, loss=0.5412739964189548\n",
      "Current iteration=400, loss=0.5411155008625274\n",
      "Current iteration=500, loss=0.5410624806793088\n",
      "Current iteration=600, loss=0.5410441097741868\n",
      "Current iteration=700, loss=0.5410376273364207\n",
      "Current iteration=800, loss=0.5410353169273785\n",
      "Current iteration=0, loss=0.5402675026071933\n",
      "Current iteration=100, loss=0.5401480001923136\n",
      "Current iteration=200, loss=0.5401452281728484\n",
      "Current iteration=0, loss=0.543584927798681\n",
      "Current iteration=100, loss=0.5434819360982678\n",
      "Current iteration=200, loss=0.5434777883114079\n",
      "Current iteration=0, loss=0.5408228558647676\n",
      "Current iteration=100, loss=0.5407017039047481\n",
      "Current iteration=200, loss=0.5406970817244697\n",
      "Iteration 1\n",
      "Current iteration=0, loss=0.541282253886665\n",
      "Current iteration=0, loss=0.5404414833247254\n",
      "Current iteration=100, loss=0.5403201232866919\n",
      "Current iteration=200, loss=0.5403165227125432\n",
      "Current iteration=0, loss=0.5437550329861235\n",
      "Current iteration=100, loss=0.5436514172193179\n",
      "Current iteration=200, loss=0.5436471363379148\n",
      "Current iteration=0, loss=0.5409998167191056\n",
      "Current iteration=100, loss=0.5408790588827965\n",
      "Current iteration=200, loss=0.5408745825976436\n",
      "Iteration 2\n",
      "Current iteration=0, loss=0.5415227645262563\n",
      "Current iteration=0, loss=0.5406737117276391\n",
      "Current iteration=100, loss=0.5405517514780054\n",
      "Current iteration=200, loss=0.540547936588718\n",
      "Current iteration=0, loss=0.5439839234312049\n",
      "Current iteration=100, loss=0.5438805855740941\n",
      "Current iteration=200, loss=0.543876389113701\n",
      "Current iteration=0, loss=0.5412392992433587\n",
      "Current iteration=100, loss=0.5411190588309125\n",
      "Current iteration=200, loss=0.5411147615771809\n",
      "Iteration 3\n",
      "Current iteration=0, loss=0.5418469715857781\n",
      "Current iteration=0, loss=0.540986628608338\n",
      "Current iteration=100, loss=0.5408638595352593\n",
      "Current iteration=200, loss=0.5408597820526784\n",
      "Current iteration=0, loss=0.5442923992090574\n",
      "Current iteration=100, loss=0.5441894618058059\n",
      "Current iteration=200, loss=0.5441853829031703\n",
      "Current iteration=0, loss=0.5415617059536563\n",
      "Current iteration=100, loss=0.54144214799625\n",
      "Current iteration=200, loss=0.5414380828534872\n",
      "Iteration 4\n",
      "Current iteration=0, loss=0.5422812929532067\n",
      "Current iteration=100, loss=0.5422083887253005\n",
      "Current iteration=0, loss=0.5414053878487363\n",
      "Current iteration=100, loss=0.541281709357107\n",
      "Current iteration=200, loss=0.5412773741548458\n",
      "Current iteration=0, loss=0.5447055525210563\n",
      "Current iteration=100, loss=0.5446031865715063\n",
      "Current iteration=200, loss=0.5445992755950438\n",
      "Current iteration=0, loss=0.5419929580069919\n",
      "Current iteration=100, loss=0.5418742657395698\n",
      "Current iteration=200, loss=0.5418704883980522\n",
      "Iteration 5\n",
      "Current iteration=0, loss=0.5428585286143967\n",
      "Current iteration=100, loss=0.5427828274131894\n",
      "Current iteration=0, loss=0.5419604139333204\n",
      "Current iteration=100, loss=0.5418364683928311\n",
      "Current iteration=200, loss=0.5418321095634417\n",
      "Current iteration=0, loss=0.5452545528483198\n",
      "Current iteration=100, loss=0.5451530300582141\n",
      "Current iteration=200, loss=0.5451493586898052\n",
      "Current iteration=0, loss=0.5425650865194441\n",
      "Current iteration=100, loss=0.5424474674299098\n",
      "Current iteration=200, loss=0.5424440368470441\n",
      "Iteration 6\n",
      "Current iteration=0, loss=0.5436182305050544\n",
      "Current iteration=100, loss=0.5435374867228597\n",
      "Current iteration=0, loss=0.5426889314051301\n",
      "Current iteration=100, loss=0.5425659909096792\n",
      "Current iteration=200, loss=0.5425619690915091\n",
      "Current iteration=0, loss=0.5459771374817941\n",
      "Current iteration=100, loss=0.5458767974785312\n",
      "Current iteration=200, loss=0.5458734503056866\n",
      "Current iteration=0, loss=0.5433163835744097\n",
      "Current iteration=100, loss=0.5432001674484628\n",
      "Current iteration=200, loss=0.543197164545163\n",
      "Iteration 7\n",
      "Current iteration=0, loss=0.5446063404094713\n",
      "Current iteration=100, loss=0.54451700828709\n",
      "Current iteration=200, loss=0.5445143174366397\n",
      "Current iteration=0, loss=0.5436359409599619\n",
      "Current iteration=100, loss=0.5435148433355633\n",
      "Current iteration=200, loss=0.5435113572405751\n",
      "Current iteration=0, loss=0.5469174999102442\n",
      "Current iteration=100, loss=0.5468186846331913\n",
      "Current iteration=200, loss=0.5468157473863029\n",
      "Current iteration=0, loss=0.5442915121143159\n",
      "Current iteration=100, loss=0.5441769398255015\n",
      "Current iteration=200, loss=0.5441744089828624\n",
      "Iteration 8\n",
      "Current iteration=0, loss=0.5458742730017663\n",
      "Current iteration=100, loss=0.545771013903768\n",
      "Current iteration=200, loss=0.5457668524614218\n",
      "Current iteration=0, loss=0.5448519673264243\n",
      "Current iteration=100, loss=0.5447329588305266\n",
      "Current iteration=200, loss=0.5447300534419975\n",
      "Current iteration=0, loss=0.5481254497556958\n",
      "Current iteration=100, loss=0.5480284801126221\n",
      "Current iteration=0, loss=0.5455400721648663\n",
      "Current iteration=100, loss=0.5454274653517459\n",
      "Iteration 9\n",
      "Current iteration=0, loss=0.54747698647008\n",
      "Current iteration=100, loss=0.5473524983456501\n",
      "Current iteration=200, loss=0.5473468276750661\n",
      "Current iteration=0, loss=0.546390995645493\n",
      "Current iteration=100, loss=0.5462740062562325\n",
      "Current iteration=0, loss=0.5496549520842801\n",
      "Current iteration=100, loss=0.5495601125399785\n",
      "Current iteration=0, loss=0.547115569309423\n",
      "Current iteration=100, loss=0.5470052078082034\n",
      "Iteration 10\n",
      "Current iteration=0, loss=0.5494712744618595\n",
      "Current iteration=100, loss=0.5493162924700606\n",
      "Current iteration=200, loss=0.5493095659756593\n",
      "Current iteration=0, loss=0.5483091010100283\n",
      "Current iteration=100, loss=0.5481937494253606\n",
      "Current iteration=0, loss=0.5515621946127455\n",
      "Current iteration=100, loss=0.5514697019166422\n",
      "Current iteration=0, loss=0.5490733817125322\n",
      "Current iteration=100, loss=0.5489656093624609\n",
      "Iteration 11\n",
      "Current iteration=0, loss=0.551913346908137\n",
      "Current iteration=100, loss=0.5517174251709572\n",
      "Current iteration=200, loss=0.5517105871934807\n",
      "Current iteration=0, loss=0.5506623041138489\n",
      "Current iteration=100, loss=0.5505485145231989\n",
      "Current iteration=0, loss=0.5539030814792439\n",
      "Current iteration=100, loss=0.5538131925105301\n",
      "Current iteration=0, loss=0.5514688905593649\n",
      "Current iteration=100, loss=0.5513641045280816\n",
      "Iteration 12\n",
      "Current iteration=0, loss=0.5548569563189586\n",
      "Current iteration=100, loss=0.5546096523017761\n",
      "Current iteration=200, loss=0.5546038107185675\n",
      "Current iteration=0, loss=0.5535057858740785\n",
      "Current iteration=100, loss=0.553394449950483\n",
      "Current iteration=0, loss=0.556731490776756\n",
      "Current iteration=100, loss=0.5566446625939137\n",
      "Current iteration=0, loss=0.5543558322784158\n",
      "Current iteration=100, loss=0.5542545896186786\n",
      "Iteration 13\n",
      "Current iteration=0, loss=0.5583529838560595\n",
      "Current iteration=100, loss=0.5580452265513142\n",
      "Current iteration=0, loss=0.556894841247366\n",
      "Current iteration=100, loss=0.5567873324227153\n",
      "Current iteration=0, loss=0.5600995064182526\n",
      "Current iteration=100, loss=0.5600163388385482\n",
      "Current iteration=0, loss=0.557786804589653\n",
      "Iteration 14\n",
      "Current iteration=0, loss=0.5624507744699825\n",
      "Current iteration=100, loss=0.5620760016825486\n",
      "Current iteration=0, loss=0.5608846894152215\n",
      "Current iteration=0, loss=0.5640581336441148\n",
      "Current iteration=0, loss=0.5618142257469873\n",
      "Iteration 15\n",
      "Current iteration=0, loss=0.5671988212269967\n",
      "Current iteration=100, loss=0.566752831670242\n",
      "Current iteration=0, loss=0.5655287143018419\n",
      "Current iteration=0, loss=0.5686563834662267\n",
      "Current iteration=0, loss=0.5664897168760162\n",
      "Iteration 16\n",
      "Current iteration=0, loss=0.5726410478246675\n",
      "Current iteration=100, loss=0.5721219564288821\n",
      "Current iteration=0, loss=0.5708751925420055\n",
      "Current iteration=0, loss=0.5739382290172993\n",
      "Current iteration=0, loss=0.5718603352641545\n",
      "Iteration 17\n",
      "Current iteration=0, loss=0.5788105293708457\n",
      "Current iteration=0, loss=0.5769613991668557\n",
      "Current iteration=0, loss=0.579937186495651\n",
      "Current iteration=0, loss=0.5779622163484637\n",
      "Iteration 18\n",
      "Current iteration=0, loss=0.5857198386277152\n",
      "Current iteration=0, loss=0.5838053422569078\n",
      "Current iteration=0, loss=0.5866678487951622\n",
      "Current iteration=0, loss=0.5848114007047355\n",
      "Iteration 19\n",
      "Current iteration=0, loss=0.5933477204309313\n",
      "Current iteration=0, loss=0.5913930545408477\n",
      "Current iteration=0, loss=0.594113857643117\n",
      "Current iteration=0, loss=0.5923910982508146\n",
      "Iteration 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.6016242403604695\n",
      "Current iteration=0, loss=0.5996644473741906\n",
      "Current iteration=0, loss=0.6022146031987679\n",
      "Current iteration=0, loss=0.6006384527142085\n",
      "Iteration 21\n",
      "Current iteration=0, loss=0.6104185603239362\n",
      "Current iteration=0, loss=0.6085011713752814\n",
      "Current iteration=0, loss=0.6108532019224431\n",
      "Current iteration=0, loss=0.6094331119786355\n",
      "Iteration 22\n",
      "Current iteration=0, loss=0.6195364315050962\n",
      "Current iteration=0, loss=0.6177225657193011\n",
      "Current iteration=0, loss=0.6198532453458239\n",
      "Current iteration=0, loss=0.6185942077400954\n",
      "Iteration 23\n",
      "Current iteration=0, loss=0.6287322780810444\n",
      "Current iteration=0, loss=0.6270941773859069\n",
      "Current iteration=0, loss=0.6289866211974747\n",
      "Current iteration=0, loss=0.6278888565461147\n",
      "Iteration 24\n",
      "Current iteration=0, loss=0.6377404249206915\n",
      "Current iteration=0, loss=0.6363483422177253\n",
      "Current iteration=0, loss=0.6379937117279952\n",
      "Current iteration=0, loss=0.6370533702378707\n",
      "Iteration 25\n",
      "Current iteration=0, loss=0.6463141872612941\n",
      "Current iteration=0, loss=0.6452143247575414\n",
      "Current iteration=0, loss=0.6466129931843535\n",
      "Current iteration=0, loss=0.6458220304850393\n",
      "Iteration 26\n",
      "Current iteration=0, loss=0.6542707112990153\n",
      "Current iteration=0, loss=0.6534505116377355\n",
      "Current iteration=0, loss=0.6546117214801909\n",
      "Current iteration=0, loss=0.653958719927162\n",
      "Iteration 27\n",
      "Current iteration=0, loss=0.6615275439410324\n",
      "Current iteration=0, loss=0.6608713533767255\n",
      "Current iteration=0, loss=0.6618120010379072\n",
      "Current iteration=0, loss=0.6612825990614832\n",
      "Iteration 28\n",
      "Current iteration=0, loss=0.6681277450840555\n",
      "Current iteration=0, loss=0.6673637236097636\n",
      "Current iteration=0, loss=0.6681060370716809\n",
      "Current iteration=0, loss=0.6676839319419885\n",
      "Iteration 29\n",
      "Current iteration=0, loss=0.674231384552687\n",
      "Current iteration=100, loss=0.9670563837458352\n",
      "Current iteration=200, loss=0.9670563838049002\n",
      "Current iteration=300, loss=0.9670563838049002\n",
      "Current iteration=400, loss=0.9670563838049002\n",
      "Current iteration=500, loss=0.9670563838049002\n",
      "Current iteration=600, loss=0.9670563838049002\n",
      "Current iteration=700, loss=0.9670563838049002\n",
      "Current iteration=800, loss=0.9670563838049002\n",
      "Current iteration=900, loss=0.9670563838049002\n",
      "Current iteration=1000, loss=0.9670563838049002\n",
      "Current iteration=1100, loss=0.9670563838049002\n",
      "Current iteration=1200, loss=0.9670563838049002\n",
      "Current iteration=1300, loss=0.9670563838049002\n",
      "Current iteration=1400, loss=0.9670563838049002\n",
      "Current iteration=1500, loss=0.9670563838049002\n",
      "Current iteration=1600, loss=0.9670563838049002\n",
      "Current iteration=1700, loss=0.9670563838049002\n",
      "Current iteration=1800, loss=0.9670563838049002\n",
      "Current iteration=1900, loss=0.9670563838049002\n",
      "Current iteration=0, loss=0.964730734028153\n",
      "Current iteration=100, loss=0.9686355366561273\n",
      "Current iteration=200, loss=0.9686355366561586\n",
      "Current iteration=300, loss=0.9686355366561586\n",
      "Current iteration=400, loss=0.9686355366561586\n",
      "Current iteration=500, loss=0.9686355366561586\n",
      "Current iteration=600, loss=0.9686355366561586\n",
      "Current iteration=700, loss=0.9686355366561586\n",
      "Current iteration=800, loss=0.9686355366561586\n",
      "Current iteration=900, loss=0.9686355366561586\n",
      "Current iteration=1000, loss=0.9686355366561586\n",
      "Current iteration=1100, loss=0.9686355366561586\n",
      "Current iteration=1200, loss=0.9686355366561586\n",
      "Current iteration=1300, loss=0.9686355366561586\n",
      "Current iteration=1400, loss=0.9686355366561586\n",
      "Current iteration=1500, loss=0.9686355366561586\n",
      "Current iteration=1600, loss=0.9686355366561586\n",
      "Current iteration=1700, loss=0.9686355366561586\n",
      "Current iteration=1800, loss=0.9686355366561586\n",
      "Current iteration=1900, loss=0.9686355366561586\n",
      "Current iteration=0, loss=0.9709871707031258\n",
      "Current iteration=100, loss=0.9732761677613053\n",
      "Current iteration=200, loss=0.9732761677612753\n",
      "Current iteration=300, loss=0.9732761677612753\n",
      "Current iteration=400, loss=0.9732761677612753\n",
      "Current iteration=500, loss=0.9732761677612753\n",
      "Current iteration=600, loss=0.9732761677612753\n",
      "Current iteration=700, loss=0.9732761677612753\n",
      "Current iteration=800, loss=0.9732761677612753\n",
      "Current iteration=900, loss=0.9732761677612753\n",
      "Current iteration=1000, loss=0.9732761677612753\n",
      "Current iteration=1100, loss=0.9732761677612753\n",
      "Current iteration=1200, loss=0.9732761677612753\n",
      "Current iteration=1300, loss=0.9732761677612753\n",
      "Current iteration=1400, loss=0.9732761677612753\n",
      "Current iteration=1500, loss=0.9732761677612753\n",
      "Current iteration=1600, loss=0.9732761677612753\n",
      "Current iteration=1700, loss=0.9732761677612753\n",
      "Current iteration=1800, loss=0.9732761677612753\n",
      "Current iteration=1900, loss=0.9732761677612753\n",
      "Current iteration=0, loss=0.9709272266435132\n",
      "Current iteration=100, loss=0.966919672871544\n",
      "Current iteration=200, loss=0.9669196728714868\n",
      "Current iteration=300, loss=0.9669196728714868\n",
      "Current iteration=400, loss=0.9669196728714868\n",
      "Current iteration=500, loss=0.9669196728714868\n",
      "Current iteration=600, loss=0.9669196728714868\n",
      "Current iteration=700, loss=0.9669196728714868\n",
      "Current iteration=800, loss=0.9669196728714868\n",
      "Current iteration=900, loss=0.9669196728714868\n",
      "Current iteration=1000, loss=0.9669196728714868\n",
      "Current iteration=1100, loss=0.9669196728714868\n",
      "Current iteration=1200, loss=0.9669196728714868\n",
      "Current iteration=1300, loss=0.9669196728714868\n",
      "Current iteration=1400, loss=0.9669196728714868\n",
      "Current iteration=1500, loss=0.9669196728714868\n",
      "Current iteration=1600, loss=0.9669196728714868\n",
      "Current iteration=1700, loss=0.9669196728714868\n",
      "Current iteration=1800, loss=0.9669196728714868\n",
      "Current iteration=1900, loss=0.9669196728714868\n",
      "Iteration 0\n",
      "Current iteration=0, loss=0.6054504860332675\n",
      "Current iteration=100, loss=0.5284642005880328\n",
      "Current iteration=200, loss=0.5261497396016063\n",
      "Current iteration=300, loss=0.5256326736678651\n",
      "Current iteration=400, loss=0.525481283388357\n",
      "Current iteration=500, loss=0.5254303468429427\n",
      "Current iteration=600, loss=0.5254115195509004\n",
      "Current iteration=700, loss=0.525404051685491\n",
      "Current iteration=800, loss=0.5254009107571885\n",
      "Current iteration=0, loss=0.5215513469654015\n",
      "Current iteration=100, loss=0.52132080896708\n",
      "Current iteration=200, loss=0.5213133745486195\n",
      "Current iteration=0, loss=0.5258679398297832\n",
      "Current iteration=100, loss=0.5256454085807435\n",
      "Current iteration=200, loss=0.5256395012562007\n",
      "Current iteration=0, loss=0.525271822066123\n",
      "Current iteration=100, loss=0.5251360090088949\n",
      "Current iteration=200, loss=0.525133246736115\n",
      "Iteration 1\n",
      "Current iteration=0, loss=0.5258092660808921\n",
      "Current iteration=100, loss=0.5255820008798153\n",
      "Current iteration=200, loss=0.5255775240688034\n",
      "Current iteration=0, loss=0.5217294853758554\n",
      "Current iteration=100, loss=0.5215032526433486\n",
      "Current iteration=200, loss=0.5214966027743232\n",
      "Current iteration=0, loss=0.5260451439298928\n",
      "Current iteration=100, loss=0.525823711006287\n",
      "Current iteration=200, loss=0.5258180664977252\n",
      "Current iteration=0, loss=0.5254486870483674\n",
      "Current iteration=100, loss=0.5253136209376957\n",
      "Current iteration=200, loss=0.5253109512442795\n",
      "Iteration 2\n",
      "Current iteration=0, loss=0.5260496707366924\n",
      "Current iteration=100, loss=0.525822183530259\n",
      "Current iteration=200, loss=0.5258175736554879\n",
      "Current iteration=0, loss=0.5219767560010429\n",
      "Current iteration=100, loss=0.5217514794523666\n",
      "Current iteration=200, loss=0.5217449689659289\n",
      "Current iteration=0, loss=0.5262857232797212\n",
      "Current iteration=100, loss=0.5260653814100728\n",
      "Current iteration=200, loss=0.5260599165023943\n",
      "Current iteration=0, loss=0.5256883544241507\n",
      "Current iteration=100, loss=0.5255541557128979\n",
      "Current iteration=200, loss=0.5255515647249278\n",
      "Iteration 3\n",
      "Current iteration=0, loss=0.5263737780043894\n",
      "Current iteration=100, loss=0.526145767596223\n",
      "Current iteration=200, loss=0.5261409553403078\n",
      "Current iteration=0, loss=0.5223098965352748\n",
      "Current iteration=100, loss=0.5220858916429598\n",
      "Current iteration=200, loss=0.5220795603895954\n",
      "Current iteration=0, loss=0.5266098825226008\n",
      "Current iteration=100, loss=0.5263909602489594\n",
      "Current iteration=200, loss=0.526385723421983\n",
      "Current iteration=0, loss=0.5260111864398022\n",
      "Current iteration=100, loss=0.5258781478229483\n",
      "Iteration 4\n",
      "Current iteration=0, loss=0.5268081844752774\n",
      "Current iteration=100, loss=0.5265789997750384\n",
      "Current iteration=200, loss=0.5265738316266594\n",
      "Current iteration=0, loss=0.522755856644972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=100, loss=0.5225334822455865\n",
      "Current iteration=200, loss=0.5225273780677501\n",
      "Current iteration=0, loss=0.5270437105186521\n",
      "Current iteration=100, loss=0.5268267202610146\n",
      "Current iteration=200, loss=0.5268217841161486\n",
      "Current iteration=0, loss=0.526443195039386\n",
      "Current iteration=100, loss=0.5263117037869326\n",
      "Iteration 5\n",
      "Current iteration=0, loss=0.5273858119125454\n",
      "Current iteration=100, loss=0.5271542534089363\n",
      "Current iteration=200, loss=0.5271484953936483\n",
      "Current iteration=0, loss=0.5233479484207131\n",
      "Current iteration=100, loss=0.5231275967932932\n",
      "Current iteration=200, loss=0.5231217729946952\n",
      "Current iteration=0, loss=0.5276197061520089\n",
      "Current iteration=100, loss=0.5274051649294185\n",
      "Current iteration=200, loss=0.5274005925185773\n",
      "Current iteration=0, loss=0.5270165262444391\n",
      "Current iteration=100, loss=0.5268870479854165\n",
      "Iteration 6\n",
      "Current iteration=0, loss=0.5281460888694698\n",
      "Current iteration=100, loss=0.527910174829217\n",
      "Current iteration=200, loss=0.5279035195387566\n",
      "Current iteration=0, loss=0.524125751266114\n",
      "Current iteration=100, loss=0.5239079970815262\n",
      "Current iteration=200, loss=0.5239025433219238\n",
      "Current iteration=0, loss=0.5283763505825515\n",
      "Current iteration=100, loss=0.5281650452980258\n",
      "Current iteration=200, loss=0.5281609285485813\n",
      "Current iteration=0, loss=0.5277695004209838\n",
      "Current iteration=100, loss=0.5276426102504556\n",
      "Iteration 7\n",
      "Current iteration=0, loss=0.5291345451409991\n",
      "Current iteration=100, loss=0.5288909906388738\n",
      "Current iteration=200, loss=0.5288830904976327\n",
      "Current iteration=0, loss=0.5251346163338596\n",
      "Current iteration=100, loss=0.5249201846479147\n",
      "Current iteration=200, loss=0.5249152214441509\n",
      "Current iteration=0, loss=0.5293580483726599\n",
      "Current iteration=100, loss=0.5291507835316073\n",
      "Current iteration=200, loss=0.5291471967894417\n",
      "Current iteration=0, loss=0.5287459179965351\n",
      "Current iteration=100, loss=0.52862229786449\n",
      "Iteration 8\n",
      "Current iteration=0, loss=0.5304007877836245\n",
      "Current iteration=100, loss=0.5301446939945754\n",
      "Current iteration=200, loss=0.5301353013519458\n",
      "Current iteration=0, loss=0.5264237856500613\n",
      "Current iteration=100, loss=0.5262135686775178\n",
      "Current iteration=200, loss=0.526209240139429\n",
      "Current iteration=0, loss=0.5306129954853294\n",
      "Current iteration=100, loss=0.530410641508569\n",
      "Current iteration=0, loss=0.5299933116416686\n",
      "Current iteration=100, loss=0.5298737273075351\n",
      "Iteration 9\n",
      "Current iteration=0, loss=0.5319954959576948\n",
      "Current iteration=100, loss=0.5317202590182187\n",
      "Current iteration=200, loss=0.531709462524473\n",
      "Current iteration=0, loss=0.5280436689277305\n",
      "Current iteration=100, loss=0.5278385660979547\n",
      "Current iteration=200, loss=0.5278349959531862\n",
      "Current iteration=0, loss=0.5321905601597082\n",
      "Current iteration=100, loss=0.5319939530909011\n",
      "Current iteration=0, loss=0.5315601421177387\n",
      "Current iteration=100, loss=0.5314454107126926\n",
      "Iteration 10\n",
      "Current iteration=0, loss=0.5339669100877509\n",
      "Current iteration=100, loss=0.5336643745059448\n",
      "Current iteration=200, loss=0.5336528370139467\n",
      "Current iteration=0, loss=0.5300421419869483\n",
      "Current iteration=100, loss=0.5298429481122585\n",
      "Current iteration=0, loss=0.5341378823286149\n",
      "Current iteration=100, loss=0.5339477074797275\n",
      "Current iteration=0, loss=0.5334924407751285\n",
      "Current iteration=100, loss=0.5333833723492665\n",
      "Iteration 11\n",
      "Current iteration=0, loss=0.5363569734162321\n",
      "Current iteration=100, loss=0.5360185090363588\n",
      "Current iteration=200, loss=0.5360075144396885\n",
      "Current iteration=0, loss=0.5324610651721449\n",
      "Current iteration=100, loss=0.5322683823546087\n",
      "Current iteration=0, loss=0.5364966122066512\n",
      "Current iteration=100, loss=0.5363133181207598\n",
      "Current iteration=0, loss=0.5358306682835053\n",
      "Current iteration=100, loss=0.5357280252135324\n",
      "Iteration 12\n",
      "Current iteration=0, loss=0.5391991412454504\n",
      "Current iteration=100, loss=0.5388171561338782\n",
      "Current iteration=200, loss=0.5388082020951528\n",
      "Current iteration=0, loss=0.5353339683534785\n",
      "Current iteration=100, loss=0.5351482465944878\n",
      "Current iteration=0, loss=0.5393006458958898\n",
      "Current iteration=100, loss=0.5391245059488918\n",
      "Current iteration=0, loss=0.5386078774837754\n",
      "Current iteration=100, loss=0.5385123051457494\n",
      "Iteration 13\n",
      "Current iteration=0, loss=0.5425178196013813\n",
      "Current iteration=100, loss=0.5420875745035502\n",
      "Current iteration=0, loss=0.5386859797348238\n",
      "Current iteration=100, loss=0.5385077028962071\n",
      "Current iteration=0, loss=0.5425756092782681\n",
      "Current iteration=100, loss=0.5424070477566298\n",
      "Current iteration=0, loss=0.5418498142553003\n",
      "Iteration 14\n",
      "Current iteration=0, loss=0.546330787792311\n",
      "Current iteration=100, loss=0.5458512886790654\n",
      "Current iteration=0, loss=0.542536533129226\n",
      "Current iteration=100, loss=0.542366487549185\n",
      "Current iteration=0, loss=0.5463412015109383\n",
      "Current iteration=0, loss=0.5455774638167187\n",
      "Iteration 15\n",
      "Current iteration=0, loss=0.5506533755699416\n",
      "Current iteration=100, loss=0.5501277104395588\n",
      "Current iteration=0, loss=0.5469046689263852\n",
      "Current iteration=0, loss=0.5506157078470999\n",
      "Current iteration=0, loss=0.5498118313229633\n",
      "Iteration 16\n",
      "Current iteration=0, loss=0.5555052202251513\n",
      "Current iteration=100, loss=0.5549395889000877\n",
      "Current iteration=0, loss=0.5518155188671342\n",
      "Current iteration=0, loss=0.555422085984964\n",
      "Current iteration=0, loss=0.554579951174695\n",
      "Iteration 17\n",
      "Current iteration=0, loss=0.560915927740414\n",
      "Current iteration=0, loss=0.557305913634455\n",
      "Current iteration=0, loss=0.5607940460066188\n",
      "Current iteration=0, loss=0.5599196978687142\n",
      "Iteration 18\n",
      "Current iteration=0, loss=0.5669275817454243\n",
      "Current iteration=0, loss=0.5634259897587045\n",
      "Current iteration=0, loss=0.5667779021128329\n",
      "Current iteration=0, loss=0.5658812994922173\n",
      "Iteration 19\n",
      "Current iteration=0, loss=0.5735904812697709\n",
      "Current iteration=0, loss=0.5702331825635109\n",
      "Current iteration=0, loss=0.5734276790414704\n",
      "Current iteration=0, loss=0.5725217993608595\n",
      "Iteration 20\n",
      "Current iteration=0, loss=0.5809492339182987\n",
      "Current iteration=0, loss=0.577777776919917\n",
      "Current iteration=0, loss=0.5807912808386443\n",
      "Current iteration=0, loss=0.579891410537509\n",
      "Iteration 21\n",
      "Current iteration=0, loss=0.5890206009285812\n",
      "Current iteration=0, loss=0.5860814388464053\n",
      "Current iteration=0, loss=0.588890193400448\n",
      "Current iteration=0, loss=0.5880126253579292\n",
      "Iteration 22\n",
      "Current iteration=0, loss=0.597769869771617\n",
      "Current iteration=0, loss=0.5951136877301793\n",
      "Current iteration=0, loss=0.5976957034883383\n",
      "Current iteration=0, loss=0.5968567836753863\n",
      "Iteration 23\n",
      "Current iteration=0, loss=0.6070923241441516\n",
      "Current iteration=0, loss=0.6047718439942007\n",
      "Current iteration=0, loss=0.6071088267607571\n",
      "Current iteration=0, loss=0.6063240917586316\n",
      "Iteration 24\n",
      "Current iteration=0, loss=0.6168087940969449\n",
      "Current iteration=0, loss=0.6148709228527819\n",
      "Current iteration=0, loss=0.6169506173194483\n",
      "Current iteration=0, loss=0.6162330693956715\n",
      "Iteration 25\n",
      "Current iteration=0, loss=0.6266838526286224\n",
      "Current iteration=0, loss=0.6251485057602566\n",
      "Current iteration=0, loss=0.6269656884617061\n",
      "Current iteration=0, loss=0.6263251075612767\n",
      "Iteration 26\n",
      "Current iteration=0, loss=0.6364658440437941\n",
      "Current iteration=0, loss=0.6352874556895521\n",
      "Current iteration=0, loss=0.6368450879499813\n",
      "Current iteration=0, loss=0.6362868330442074\n",
      "Iteration 27\n",
      "Current iteration=0, loss=0.6459493557367845\n",
      "Current iteration=0, loss=0.6449549215207796\n",
      "Current iteration=0, loss=0.6462642032666589\n",
      "Current iteration=0, loss=0.645788714519828\n",
      "Iteration 28\n",
      "Current iteration=0, loss=0.6550459562417046\n",
      "Current iteration=100, loss=0.6705698398136395\n",
      "Current iteration=200, loss=0.6843066458719932\n",
      "Current iteration=300, loss=0.6843071263472588\n",
      "Current iteration=400, loss=0.6843071263568269\n",
      "Current iteration=500, loss=0.684307126356827\n",
      "Current iteration=600, loss=0.684307126356827\n",
      "Current iteration=700, loss=0.684307126356827\n",
      "Current iteration=800, loss=0.684307126356827\n",
      "Current iteration=900, loss=0.684307126356827\n",
      "Current iteration=1000, loss=0.684307126356827\n",
      "Current iteration=1100, loss=0.684307126356827\n",
      "Current iteration=1200, loss=0.684307126356827\n",
      "Current iteration=1300, loss=0.684307126356827\n",
      "Current iteration=1400, loss=0.684307126356827\n",
      "Current iteration=1500, loss=0.684307126356827\n",
      "Current iteration=1600, loss=0.684307126356827\n",
      "Current iteration=1700, loss=0.684307126356827\n",
      "Current iteration=1800, loss=0.684307126356827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=1900, loss=0.684307126356827\n",
      "Current iteration=0, loss=0.6838076672512111\n",
      "Current iteration=100, loss=0.682300639963849\n",
      "Current iteration=200, loss=0.6823005961663221\n",
      "Current iteration=300, loss=0.6823005961649768\n",
      "Current iteration=400, loss=0.6823005961649768\n",
      "Current iteration=500, loss=0.6823005961649768\n",
      "Current iteration=600, loss=0.6823005961649768\n",
      "Current iteration=700, loss=0.6823005961649768\n",
      "Current iteration=800, loss=0.6823005961649768\n",
      "Current iteration=900, loss=0.6823005961649768\n",
      "Current iteration=1000, loss=0.6823005961649768\n",
      "Current iteration=1100, loss=0.6823005961649768\n",
      "Current iteration=1200, loss=0.6823005961649768\n",
      "Current iteration=1300, loss=0.6823005961649768\n",
      "Current iteration=1400, loss=0.6823005961649768\n",
      "Current iteration=1500, loss=0.6823005961649768\n",
      "Current iteration=1600, loss=0.6823005961649768\n",
      "Current iteration=1700, loss=0.6823005961649768\n",
      "Current iteration=1800, loss=0.6823005961649768\n",
      "Current iteration=1900, loss=0.6823005961649768\n",
      "Current iteration=0, loss=0.6849485994718729\n",
      "Current iteration=100, loss=0.6889924676047269\n",
      "Current iteration=200, loss=0.6889924865114807\n",
      "Current iteration=300, loss=0.6889924865115595\n",
      "Current iteration=400, loss=0.6889924865115595\n",
      "Current iteration=500, loss=0.6889924865115595\n",
      "Current iteration=600, loss=0.6889924865115595\n",
      "Current iteration=700, loss=0.6889924865115595\n",
      "Current iteration=800, loss=0.6889924865115595\n",
      "Current iteration=900, loss=0.6889924865115595\n",
      "Current iteration=1000, loss=0.6889924865115595\n",
      "Current iteration=1100, loss=0.6889924865115595\n",
      "Current iteration=1200, loss=0.6889924865115595\n",
      "Current iteration=1300, loss=0.6889924865115595\n",
      "Current iteration=1400, loss=0.6889924865115595\n",
      "Current iteration=1500, loss=0.6889924865115595\n",
      "Current iteration=1600, loss=0.6889924865115595\n",
      "Current iteration=1700, loss=0.6889924865115595\n",
      "Current iteration=1800, loss=0.6889924865115595\n",
      "Current iteration=1900, loss=0.6889924865115595\n",
      "Current iteration=0, loss=0.6858780072319322\n",
      "Current iteration=100, loss=0.6755687861385792\n",
      "Current iteration=200, loss=0.6755666249638627\n",
      "Current iteration=300, loss=0.6755666243079209\n",
      "Current iteration=400, loss=0.6755666243077222\n",
      "Current iteration=500, loss=0.675566624307722\n",
      "Current iteration=600, loss=0.675566624307722\n",
      "Current iteration=700, loss=0.675566624307722\n",
      "Current iteration=800, loss=0.675566624307722\n",
      "Current iteration=900, loss=0.675566624307722\n",
      "Current iteration=1000, loss=0.675566624307722\n",
      "Current iteration=1100, loss=0.675566624307722\n",
      "Current iteration=1200, loss=0.675566624307722\n",
      "Current iteration=1300, loss=0.675566624307722\n",
      "Current iteration=1400, loss=0.675566624307722\n",
      "Current iteration=1500, loss=0.675566624307722\n",
      "Current iteration=1600, loss=0.675566624307722\n",
      "Current iteration=1700, loss=0.675566624307722\n",
      "Current iteration=1800, loss=0.675566624307722\n",
      "Current iteration=1900, loss=0.675566624307722\n",
      "Iteration 29\n",
      "Current iteration=0, loss=0.7131852243312183\n",
      "Current iteration=100, loss=1.5457952276692366\n",
      "Current iteration=200, loss=1.545795227669259\n",
      "Current iteration=300, loss=1.545795227669259\n",
      "Current iteration=400, loss=1.545795227669259\n",
      "Current iteration=500, loss=1.545795227669259\n",
      "Current iteration=600, loss=1.545795227669259\n",
      "Current iteration=700, loss=1.545795227669259\n",
      "Current iteration=800, loss=1.545795227669259\n",
      "Current iteration=900, loss=1.545795227669259\n",
      "Current iteration=1000, loss=1.545795227669259\n",
      "Current iteration=1100, loss=1.545795227669259\n",
      "Current iteration=1200, loss=1.545795227669259\n",
      "Current iteration=1300, loss=1.545795227669259\n",
      "Current iteration=1400, loss=1.545795227669259\n",
      "Current iteration=1500, loss=1.545795227669259\n",
      "Current iteration=1600, loss=1.545795227669259\n",
      "Current iteration=1700, loss=1.545795227669259\n",
      "Current iteration=1800, loss=1.545795227669259\n",
      "Current iteration=1900, loss=1.545795227669259\n",
      "Current iteration=0, loss=1.547220173827671\n",
      "Current iteration=100, loss=1.5450860852557133\n",
      "Current iteration=200, loss=1.545086085255714\n",
      "Current iteration=300, loss=1.545086085255714\n",
      "Current iteration=400, loss=1.545086085255714\n",
      "Current iteration=500, loss=1.545086085255714\n",
      "Current iteration=600, loss=1.545086085255714\n",
      "Current iteration=700, loss=1.545086085255714\n",
      "Current iteration=800, loss=1.545086085255714\n",
      "Current iteration=900, loss=1.545086085255714\n",
      "Current iteration=1000, loss=1.545086085255714\n",
      "Current iteration=1100, loss=1.545086085255714\n",
      "Current iteration=1200, loss=1.545086085255714\n",
      "Current iteration=1300, loss=1.545086085255714\n",
      "Current iteration=1400, loss=1.545086085255714\n",
      "Current iteration=1500, loss=1.545086085255714\n",
      "Current iteration=1600, loss=1.545086085255714\n",
      "Current iteration=1700, loss=1.545086085255714\n",
      "Current iteration=1800, loss=1.545086085255714\n",
      "Current iteration=1900, loss=1.545086085255714\n",
      "Current iteration=0, loss=1.5538525079576764\n",
      "Current iteration=100, loss=1.5553273787448472\n",
      "Current iteration=200, loss=1.5553273787448463\n",
      "Current iteration=300, loss=1.5553273787448463\n",
      "Current iteration=400, loss=1.5553273787448463\n",
      "Current iteration=500, loss=1.5553273787448463\n",
      "Current iteration=600, loss=1.5553273787448463\n",
      "Current iteration=700, loss=1.5553273787448463\n",
      "Current iteration=800, loss=1.5553273787448463\n",
      "Current iteration=900, loss=1.5553273787448463\n",
      "Current iteration=1000, loss=1.5553273787448463\n",
      "Current iteration=1100, loss=1.5553273787448463\n",
      "Current iteration=1200, loss=1.5553273787448463\n",
      "Current iteration=1300, loss=1.5553273787448463\n",
      "Current iteration=1400, loss=1.5553273787448463\n",
      "Current iteration=1500, loss=1.5553273787448463\n",
      "Current iteration=1600, loss=1.5553273787448463\n",
      "Current iteration=1700, loss=1.5553273787448463\n",
      "Current iteration=1800, loss=1.5553273787448463\n",
      "Current iteration=1900, loss=1.5553273787448463\n",
      "Current iteration=0, loss=1.5350494209376904\n",
      "Current iteration=100, loss=1.5165577250162383\n",
      "Current iteration=200, loss=1.516557725016241\n",
      "Current iteration=300, loss=1.516557725016241\n",
      "Current iteration=400, loss=1.516557725016241\n",
      "Current iteration=500, loss=1.516557725016241\n",
      "Current iteration=600, loss=1.516557725016241\n",
      "Current iteration=700, loss=1.516557725016241\n",
      "Current iteration=800, loss=1.516557725016241\n",
      "Current iteration=900, loss=1.516557725016241\n",
      "Current iteration=1000, loss=1.516557725016241\n",
      "Current iteration=1100, loss=1.516557725016241\n",
      "Current iteration=1200, loss=1.516557725016241\n",
      "Current iteration=1300, loss=1.516557725016241\n",
      "Current iteration=1400, loss=1.516557725016241\n",
      "Current iteration=1500, loss=1.516557725016241\n",
      "Current iteration=1600, loss=1.516557725016241\n",
      "Current iteration=1700, loss=1.516557725016241\n",
      "Current iteration=1800, loss=1.516557725016241\n",
      "Current iteration=1900, loss=1.516557725016241\n"
     ]
    }
   ],
   "source": [
    "lambdas = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    initial_w = np.zeros(standardized_dset.shape[1])\n",
    "    max_iters = 2000\n",
    "    gamma = 0.7 # 0.01\n",
    "    y_logistic = np.ones(ybs[jet_num].size)\n",
    "    y_logistic[ybs[jet_num] == -1] = 0\n",
    "    optimal_lambda = logistic_optimal_lambda(y_logistic, standardized_dset, initial_w, max_iters, gamma)\n",
    "    lambdas.append(optimal_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001, 0.0001, 0.00013738237958832623]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambdas when replacing -999 by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001, 0.0001, 0.0001]"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=0.5981457873646676\n",
      "Current iteration=100, loss=0.40570366954781983\n",
      "Current iteration=200, loss=0.39811158731651086\n",
      "Current iteration=300, loss=0.3951233762054761\n",
      "Current iteration=400, loss=0.3937249484158812\n",
      "Current iteration=500, loss=0.39302899842308897\n",
      "Current iteration=600, loss=0.3926703056127306\n",
      "Current iteration=700, loss=0.39248120630136046\n",
      "Current iteration=800, loss=0.3923800033816537\n",
      "Current iteration=900, loss=0.39232526283559516\n",
      "Current iteration=1000, loss=0.3922954253301242\n",
      "Current iteration=1100, loss=0.3922790701887242\n",
      "Current iteration=1200, loss=0.39227006822843535\n",
      "Current iteration=1300, loss=0.3922650984133083\n",
      "Current iteration=1400, loss=0.3922623484964606\n",
      "Current iteration=1500, loss=0.3922608243656276\n",
      "Current iteration=0, loss=0.6370622906477821\n",
      "Current iteration=100, loss=0.5440962786181257\n",
      "Current iteration=200, loss=0.5421053931684284\n",
      "Current iteration=300, loss=0.5416118282423613\n",
      "Current iteration=400, loss=0.54146090795682\n",
      "Current iteration=500, loss=0.5414110536697603\n",
      "Current iteration=600, loss=0.5413940004007834\n",
      "Current iteration=700, loss=0.5413880623987355\n",
      "Current iteration=800, loss=0.541385974816579\n",
      "Current iteration=0, loss=0.6052480026197808\n",
      "Current iteration=100, loss=0.5277363905813086\n",
      "Current iteration=200, loss=0.5253907368563153\n",
      "Current iteration=300, loss=0.524873646207783\n",
      "Current iteration=400, loss=0.5247265930618712\n",
      "Current iteration=500, loss=0.5246787258130633\n",
      "Current iteration=600, loss=0.5246616388789702\n",
      "Current iteration=700, loss=0.5246551142235027\n",
      "Current iteration=800, loss=0.5246524867060653\n"
     ]
    }
   ],
   "source": [
    "ws_RLR = []\n",
    "for jet_num, standardized_dset in enumerate(standardized_dsets):\n",
    "    initial_w = np.zeros(standardized_dset.shape[1])\n",
    "    max_iters = 2000\n",
    "    gamma = 0.7 # 0.01\n",
    "    y_logistic = np.ones(ybs[jet_num].size)\n",
    "    y_logistic[ybs[jet_num] == -1] = 0\n",
    "    loss, w_RLR = reg_logistic_regression(y_logistic, standardized_dset, lambdas[jet_num], initial_w, max_iters, gamma)\n",
    "    ws_RLR.append(w_RLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model_predictions(x_te, ws_RLR, pri_jet_num_idx, clean_features, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7599400000000001\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second score, after replacing -999 in DER_mass_MMC by the defined mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76004\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third score, after replacing -999 in DER_mass_MMC by the defined median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7593799999999999\n"
     ]
    }
   ],
   "source": [
    "compute_accuracy(y_te,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
