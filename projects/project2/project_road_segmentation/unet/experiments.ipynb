{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much epochs during which we choose to train our model. We choose 100 to compare the difference between our parameters. This allows for enough training time to compare the differences of the choice of those parameters, assuming improvements on 100 epochs hold for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "This is the choice of parameters for the baseline model. We minimize the number of weights using a smaller image size (from 400\\*400 resampled to 256\\*256), and using 1 grayscale channel to aggregate the 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=1,\n",
    "    aug=False,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original image size\n",
    "\n",
    "We try to use the original image size instead. This eliminates the resampling process, but results in a larger number of weights to train thus longer training times. Note that resampling to a smaller size condenses more \"context\" information for each pixel : the convolution kernel size is fixed, therefore a 256\\*256 resampling provides to the first convolution layer 400/256 more pixel information to the kernel (if we ignore the loss of information during resampling). Nevertheless, we evaluate our model performance on the original image size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=400,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=1,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dataset augmentation\n",
    "\n",
    "Now, we apply dataset augmentation using Keras' preprocessing ImageDataGenerator class. We use it, by default, to transform the original dataset images using rotations and flips. This could proove useful because most of the training dataset roads are axis-aligned, and tilted roads could be harder for our model to segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=1,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using red, green and blue channels\n",
    "\n",
    "Instead of using the grayscale of the training images, we train using directly the separate channels. This results in more weights to train thus longer training times, but also preserves more original informations about the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=1,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=True,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the chosen validation dataset\n",
    "\n",
    "Instead of letting the choice of the separation of the training and validation datasets to randomness, we use a custom predetermined split. This can be useful because some training images are more exotic than others, providing more useful information to our neural network. Would it happen that most/all of those be chosen for validation, this useful information would not be used to train our network. Instead, we set a training dataset with some conventional images and also some exotic images, and a validation dataset also with some conventional and exotic images. The details of the chosen indicies can be found in the common.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=1,\n",
    "    aug=True,\n",
    "    chosen_validation=True,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
