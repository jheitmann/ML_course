{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much epochs during which we choose to train our model. We choose 100 to compare the difference between our parameters. This allows for enough training time to compare the differences of the choice of those parameters, assuming improvements on 100 epochs hold for more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 1\n",
    "N_STEPS_PER_EPOCH = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model\n",
    "\n",
    "This is the choice of parameters for the baseline model. We minimize the number of weights using a smaller image size (from 400\\*400 resampled to 256\\*256), and using 1 grayscale channel to aggregate the 3 color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=False,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original image size\n",
    "\n",
    "We try to use the original image size instead. This eliminates the resampling process, but results in a larger number of weights to train thus longer training times. Note that resampling to a smaller size condenses more \"context\" information for each pixel : the convolution kernel size is fixed, therefore a 256\\*256 resampling provides to the first convolution layer 400/256 more pixel information to the kernel (if we ignore the loss of information during resampling). Nevertheless, we evaluate our model performance on the original image size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=400,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dataset augmentation\n",
    "\n",
    "Now, we apply dataset augmentation using Keras' preprocessing ImageDataGenerator class. We use it, by default, to transform the original dataset images using rotations and flips. This could proove useful because most of the training dataset roads are axis-aligned, and tilted roads could be harder for our model to segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using red, green and blue channels\n",
    "\n",
    "Instead of using the grayscale of the training images, we train using directly the separate channels. This results in more weights to train thus longer training times, but also preserves more original informations about the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=False,\n",
    "    rgb=True,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the chosen validation dataset\n",
    "\n",
    "Instead of letting the choice of the separation of the training and validation datasets to randomness, we use a custom predetermined split. This can be useful because some training images are more exotic than others, providing more useful information to our neural network. Would it happen that most/all of those be chosen for validation, this useful information would not be used to train our network. Instead, we set a training dataset with some conventional images and also some exotic images, and a validation dataset also with some conventional and exotic images. The details of the chosen indicies can be found in the common.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=True,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ImageDataGenerator parameters\n",
    "\n",
    "We try changing the parameters of ImageDataGenerator, changing rotation, adding zoom and shift. The missing pixels in each case, if any, are filled using the reflected method.\n",
    "\n",
    "### Rotation\n",
    "\n",
    "We try adding more rotation to ImageDataGenerator, from 90 to 360."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgargs = dict(    \n",
    "    rotation_range=360, # instead of 90\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    ")\n",
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=True,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\",\n",
    "    data_gen_args=dgargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shift\n",
    "\n",
    "We try using shifting in the x and y axis, with a maximum shift of 20% the width/height of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ENV] No missing folder in the environement.\n",
      "[MODEL] Using kernel initializer he_uniform with seed 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis/Documents/EPFL/ML/group/ML_course/projects/project2/project_road_segmentation/unet/model.py:87: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  model = Model(input=inputs, output=conv10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 256, 256, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 256, 256, 64) 640         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 64) 0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 128, 128, 128 73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 128, 128, 128 147584      conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 128)  0           conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 64, 64, 256)  295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 256)  0           conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 32, 32, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 32, 32, 512)  0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 16, 16, 512)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 16, 16, 1024) 4719616     max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 16, 16, 1024) 9438208     conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 16, 16, 1024) 0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 1024) 0           dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 32, 32, 512)  2097664     up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 1024) 0           dropout_3[0][0]                  \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 32, 32, 512)  4719104     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 32, 32, 512)  2359808     conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 512)  0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 64, 64, 256)  524544      up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 64, 64, 512)  0           conv2d_30[0][0]                  \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 64, 64, 256)  1179904     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 64, 64, 256)  590080      conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 128, 128, 256 0           conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 128, 128, 128 131200      up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 128, 128, 256 0           conv2d_28[0][0]                  \n",
      "                                                                 conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 128, 128, 128 295040      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 128, 128, 128 147584      conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_8 (UpSampling2D)  (None, 256, 256, 128 0           conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 256, 256, 64) 32832       up_sampling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 256, 256, 128 0           conv2d_26[0][0]                  \n",
      "                                                                 conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 256, 256, 64) 73792       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 256, 256, 64) 36928       conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 256, 256, 2)  1154        conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 256, 256, 1)  3           conv2d_47[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 31,031,685\n",
      "Trainable params: 31,031,685\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading data/split/val/image/satImage_026.png\n",
      "Loading data/split/val/image/satImage_033.png\n",
      "Loading data/split/val/image/satImage_065.png\n",
      "Loading data/split/val/image/satImage_082.png\n",
      "Loading data/split/val/image/satImage_083.png\n",
      "Loading data/split/val/image/satImage_084.png\n",
      "Loading data/split/val/image/satImage_085.png\n",
      "Loading data/split/val/image/satImage_086.png\n",
      "Loading data/split/val/image/satImage_087.png\n",
      "Loading data/split/val/image/satImage_088.png\n",
      "Loading data/split/val/image/satImage_089.png\n",
      "Loading data/split/val/image/satImage_090.png\n",
      "Loading data/split/val/image/satImage_092.png\n",
      "Loading data/split/val/image/satImage_093.png\n",
      "Loading data/split/val/image/satImage_094.png\n",
      "Loading data/split/val/image/satImage_095.png\n",
      "Loading data/split/val/image/satImage_097.png\n",
      "Loading data/split/val/image/satImage_098.png\n",
      "Loading data/split/val/image/satImage_099.png\n",
      "Loading data/split/val/image/satImage_100.png\n",
      "Loading data/split/val/label/satImage_026.png\n",
      "Loading data/split/val/label/satImage_033.png\n",
      "Loading data/split/val/label/satImage_065.png\n",
      "Loading data/split/val/label/satImage_082.png\n",
      "Loading data/split/val/label/satImage_083.png\n",
      "Loading data/split/val/label/satImage_084.png\n",
      "Loading data/split/val/label/satImage_085.png\n",
      "Loading data/split/val/label/satImage_086.png\n",
      "Loading data/split/val/label/satImage_087.png\n",
      "Loading data/split/val/label/satImage_088.png\n",
      "Loading data/split/val/label/satImage_089.png\n",
      "Loading data/split/val/label/satImage_090.png\n",
      "Loading data/split/val/label/satImage_092.png\n",
      "Loading data/split/val/label/satImage_093.png\n",
      "Loading data/split/val/label/satImage_094.png\n",
      "Loading data/split/val/label/satImage_095.png\n",
      "Loading data/split/val/label/satImage_097.png\n",
      "Loading data/split/val/label/satImage_098.png\n",
      "Loading data/split/val/label/satImage_099.png\n",
      "Loading data/split/val/label/satImage_100.png\n",
      "Training on images of size 256*256 with 1 input channel(s).\n",
      "Using augmented dataset with chosen validation for training\n",
      "Monitoring with val_acc saving metrics to results/train_history.csv\n",
      "Found 80 images belonging to 1 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "Found 0 images belonging to 1 classes.\n",
      "hdf5 name: unet_bw_256_2018-12-19_15_15_14.055643.hdf5\n",
      "Epoch 1/5\n",
      "10/10 [==============================] - 181s 18s/step - loss: 0.5368 - acc: 0.8148 - val_loss: 0.5134 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.79467, saving model to results/unet_bw_256_2018-12-19_15_15_14.055643.hdf5\n",
      "Epoch 2/5\n",
      "10/10 [==============================] - 202s 20s/step - loss: 0.5168 - acc: 0.7897 - val_loss: 0.5100 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.79467\n",
      "Epoch 3/5\n",
      "10/10 [==============================] - 204s 20s/step - loss: 0.4844 - acc: 0.8131 - val_loss: 0.5098 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.79467\n",
      "Epoch 4/5\n",
      "10/10 [==============================] - 209s 21s/step - loss: 0.5523 - acc: 0.7704 - val_loss: 0.5047 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.79467\n",
      "Epoch 5/5\n",
      "10/10 [==============================] - 211s 21s/step - loss: 0.4815 - acc: 0.8164 - val_loss: 0.5153 - val_acc: 0.7947\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.79467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results/unet_bw_256_2018-12-19_15_15_14.055643.hdf5'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgargs = dict(    \n",
    "    rotation_range=90,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    ")\n",
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=5,\n",
    "    steps_per_epoch=10,\n",
    "    aug=True,\n",
    "    chosen_validation=True,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\",\n",
    "    data_gen_args=dgargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zoom\n",
    "\n",
    "We try using zoom augmentation, of 20% the image current scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgargs = dict(    \n",
    "    rotation_range=90,\n",
    "    fill_mode='reflect',\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    zoom_range=0.2,\n",
    ")\n",
    "utrain.main(\n",
    "    img_height=256,\n",
    "    batch_size=2,\n",
    "    epochs=N_EPOCHS,\n",
    "    steps_per_epoch=N_STEPS_PER_EPOCH,\n",
    "    aug=True,\n",
    "    chosen_validation=True,\n",
    "    rgb=False,\n",
    "    pretrained_weights=None,\n",
    "    monitor=\"val_acc\",\n",
    "    root_folder=\".\",\n",
    "    data_gen_args=dgargs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
